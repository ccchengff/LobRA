{
    "variant": "canonical",
    "train_task_num": 6,
    "task": [
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 256,
            "dataset_name": "dolly",
            "context_length": 8192,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        },
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 128,
            "dataset_name": "evol_instruct",
            "context_length": 16384,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        },
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 128,
            "dataset_name": "xsum",
            "context_length": 16384,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        },
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 128,
            "dataset_name": "commitpackft",
            "context_length": 8192,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        },
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 64,
            "dataset_name": "meetingbank",
            "context_length": 16384,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        },
        {
            "rank": 16,
            "lora_alpha": 16,
            "target_modules": [
                "q_proj", "k_proj", "v_proj", "o_proj", "dense_h_to_4h", "dense_4h_to_h"
            ],
            "global_batch_size": 128,
            "dataset_name": "python_code_instructions",
            "context_length": 16384,
            "json_key": "text",
            "steps": 21,
            "epochs": 1
        }
    ]
}